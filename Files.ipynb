{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae04753",
   "metadata": {},
   "source": [
    "This document is number `3 of 5` documents I have written, based on the `LangChain Academy Deep Agents with LangGraph` course.\n",
    "\n",
    "The course itself has repositories that it provides you with. If my documents are not useful to you, I suggest you check them out.\n",
    "\n",
    "There are now several successful examples of very **capable** and **long-running** agents. They have given these agents the \n",
    "name `‚ÄúDeep Agents‚Äù` because they believe that they are completely different from previous generations of agents. In \n",
    "this course, you will learn what makes them different and build your own Deep Agent.\n",
    "\n",
    "In LangChain, a Deep Agent is built that is simple and configurable, allowing users to build long-running agents quickly and easily.\n",
    "\n",
    "In this course, you will build a **Deep Research Agent** using Deep Agent. The course is divided into seven modules.\n",
    "\n",
    "Each module includes a video lesson that introduces you to the key concepts, along with related workbooks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281af4a8",
   "metadata": {},
   "source": [
    "## Context Offloading with a Virtual Filesystem in LangGraph\n",
    "\n",
    "\n",
    "This code shows how AI agents can become much smarter by learning to save and retrieve information, just like how you might take notes during a long study session!\n",
    "\n",
    "### The Big Problem: Information Overload\n",
    "\n",
    "Imagine you're working on a really long research project. After hours of gathering information, you might:\n",
    "- Forget important details from earlier\n",
    "- Lose track of your original question  \n",
    "- Get overwhelmed by too much information at once\n",
    "\n",
    "AI agents face the same challenge! When they work on complex tasks (like researching a topic), their \"memory\" (context window) gets filled up with:\n",
    "- All the previous conversation\n",
    "- Tool call results\n",
    "- Intermediate findings\n",
    "- Background information\n",
    "\n",
    "This is called **\"context accumulation\"** like your desk getting messier as you work on a big project.\n",
    "\n",
    "### The Solution: A Virtual Filing Cabinet\n",
    "\n",
    "The solution is to teach our AI agent to use a \"filing cabinet\" - a virtual file system where it can:\n",
    "1. **Save important information** to files instead of keeping everything in memory\n",
    "2. **Read back specific information** when needed\n",
    "3. **Stay organized** by listing what files it has\n",
    "4. Keeps the LLM **context clean and focused**.\n",
    "5. **Preserves important intermediate results** (plans, notes, data) outside the prompt window.\n",
    "6. Enables sub-agents or later steps to **retrieve exact details without repeating everything**.\n",
    "\n",
    "This is like taking organized notes during research - you write things down so you don't forget them, and you can look them up later!\n",
    "\n",
    "Well, I said this to introduce the concept of context offloading, what it is and what it does for us.\n",
    "The issues we face and solve next:\n",
    "- Why context offloading matters for agents\n",
    "- How a virtual filesystem works inside LangGraph state\n",
    "- How to build and use file tools: `ls`, `read_file`, `write_file`\n",
    "- How the agent uses these tools in a simple research task\n",
    "\n",
    "We‚Äôll go in the same order as your script: concept ‚Üí state ‚Üí tools ‚Üí prompt/agent ‚Üí run and inspect.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263ce15",
   "metadata": {},
   "source": [
    "### 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b86cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, NotRequired\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.types import Command\n",
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c22d4",
   "metadata": {},
   "source": [
    "---\n",
    "### 2) Virtual Filesystem in State\n",
    "\n",
    "**How it works**:\n",
    "- `files` is a dictionary where:\n",
    "  - **Keys** = file names (like \"user_request.txt\")\n",
    "  - **Values** = file contents (the actual text)\n",
    "- `file_reducer` = a function that handles saving/updating files\n",
    "\n",
    "**Real-world analogy**: Like having a folder on your computer where:\n",
    "- The folder name is the `\"key\"` \n",
    "- The document inside is the `\"value\"`\n",
    "\n",
    "\n",
    "Instead of writing to the **real disk**, you‚Äôll maintain a **‚Äúvirtual filesystem‚Äù** stored in agent state:\n",
    "- It‚Äôs just a dictionary: `{ \"path/to/file.txt\": \"file content\" }`\n",
    "- Great for session/thread-level persistence inside a single agent run\n",
    "- Not intended for cross-session persistence\n",
    "\n",
    "You already defined the state and reducer (from your previous lesson). Recap:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84f07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reducer(left, right):\n",
    "    \"\"\"Merge two file dictionaries, with right side taking precedence.\"\"\"\n",
    "    if left is None:\n",
    "        return right\n",
    "    elif right is None:\n",
    "        return left\n",
    "    else:\n",
    "        # New values overwrite old ones\n",
    "        return {**left, **right}\n",
    "\n",
    "class DeepAgentState(AgentState):\n",
    "    \"\"\"Extended agent state with todos and a virtual filesystem.\"\"\"\n",
    "    files: Annotated[NotRequired[dict[str, str]], file_reducer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41742b",
   "metadata": {},
   "source": [
    "- `files` is a dict. The `file_reducer` merges updates:\n",
    "  - `left` = existing files\n",
    "  - `right` = new updates\n",
    "  - `{**left, **right}` means keys in `right` overwrite duplicates in `left`.\n",
    "\n",
    "**Human analogy**: When you update your notes, the new version replaces the old version.\n",
    "\n",
    "Key idea: Tools will update `files` by returning `Command(update={...})`. The reducer ensures updates merge safely.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6448ce",
   "metadata": {},
   "source": [
    "\n",
    "### 3) File Tools: ls, read_file, write_file\n",
    "\n",
    "**You‚Äôll create three tools to manage the virtual filesystem:**\n",
    "- `ls()`: List files\n",
    "- `read_file(file_path, offset=0, limit=2000)`: Read content with pagination (line numbers)\n",
    "- `write_file(file_path, content)`: Create/overwrite a file\n",
    "\n",
    "**Why tool descriptions in prompts?**\n",
    "- You pass clear, task-oriented descriptions via `description=...` to the `@tool` decorator. The LLM sees these to know when/how to use each tool.\n",
    "\n",
    "**Important details:**\n",
    "- `InjectedState`: Lets the tool access the current graph state (the LLM does not pass it).\n",
    "- `InjectedToolCallId`: Lets you create a `ToolMessage` linked to the tool call (useful for structured returns).\n",
    "- Error messages should be informative for the LLM (so it can retry smartly).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fbcb61",
   "metadata": {},
   "source": [
    "**The descriptions in the prompts below describe in detail how tools operate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5dea560",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_DESCRIPTION= \"\"\"\n",
    "| List all files in the virtual filesystem stored in agent state.                                                 ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  Shows what files currently exist in agent memory. Use this to orient yourself before other file operations     ‚îÇ\n",
    "‚îÇ  and maintain awareness of your file organization.                                                              ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  No parameters required - simply call ls() to see all available files.                                          |\n",
    "\"\"\"\n",
    "\n",
    "READ_FILE_DESCRIPTION= \"\"\"\n",
    "|  Read content from a file in the virtual filesystem with optional pagination.                                   ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  This tool returns file content with line numbers (like `cat -n`) and supports reading large files in chunks    ‚îÇ\n",
    "‚îÇ  to avoid context overflow.                                                                                     ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  Parameters:                                                                                                    ‚îÇ\n",
    "‚îÇ  - file_path (required): Path to the file you want to read                                                      ‚îÇ\n",
    "‚îÇ  - offset (optional, default=0): Line number to start reading from                                              ‚îÇ\n",
    "‚îÇ  - limit (optional, default=2000): Maximum number of lines to read                                              ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  Essential before making any edits to understand existing content. Always read a file before editing it.        | \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "WRITE_FILE_DESCRIPTION= \"\"\" \n",
    "|  Create a new file or completely overwrite an existing file in the virtual filesystem.                          ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  This tool creates new files or replaces entire file contents. Use for initial file creation or complete        ‚îÇ\n",
    "‚îÇ  rewrites. Files are stored persistently in agent state.                                                        ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  Parameters:                                                                                                    ‚îÇ\n",
    "‚îÇ  - file_path (required): Path where the file should be created/overwritten                                      ‚îÇ\n",
    "‚îÇ  - content (required): The complete content to write to the file                                                ‚îÇ\n",
    "‚îÇ                                                                                                                 ‚îÇ\n",
    "‚îÇ  Important: This replaces the entire file content. Use edit_file for partial modifications.                     |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543cf45",
   "metadata": {},
   "source": [
    "**Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac8dd1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PN\\anaconda3\\envs\\llms\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:504: UserWarning: typing.NotRequired is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "c:\\Users\\PN\\anaconda3\\envs\\llms\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:504: UserWarning: typing.NotRequired is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "c:\\Users\\PN\\anaconda3\\envs\\llms\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:504: UserWarning: typing.NotRequired is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "c:\\Users\\PN\\anaconda3\\envs\\llms\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:504: UserWarning: typing.NotRequired is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "c:\\Users\\PN\\anaconda3\\envs\\llms\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:504: UserWarning: typing.NotRequired is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "c:\\Users\\PN\\anaconda3\\envs\\llms\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:504: UserWarning: typing.NotRequired is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "@tool(description=LS_DESCRIPTION)\n",
    "def ls(state: Annotated[DeepAgentState, InjectedState]) -> list[str]:\n",
    "    \"\"\"List all files in the virtual filesystem.\"\"\"\n",
    "    return list(state.get(\"files\", {}).keys())\n",
    "\n",
    "@tool(description=READ_FILE_DESCRIPTION, parse_docstring=True)\n",
    "def read_file(\n",
    "    file_path: str,\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    offset: int = 0,\n",
    "    limit: int = 2000,\n",
    ") -> str:\n",
    "    \"\"\"Read file content from virtual filesystem with optional offset and limit.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the file to read\n",
    "        state: Agent state containing virtual filesystem (injected in tool node)\n",
    "        offset: Line number to start reading from (default: 0)\n",
    "        limit: Maximum number of lines to read (default: 2000)\n",
    "\n",
    "    Returns:\n",
    "        Formatted file content with line numbers, or error message if file not found\n",
    "    \"\"\"\n",
    "    files = state.get(\"files\", {})\n",
    "    if file_path not in files:\n",
    "        return f\"Error: File '{file_path}' not found\"\n",
    "\n",
    "    content = files[file_path]\n",
    "    if not content:\n",
    "        return \"System reminder: File exists but has empty contents\"\n",
    "\n",
    "    lines = content.splitlines()\n",
    "    start_idx = offset\n",
    "    end_idx = min(start_idx + limit, len(lines))\n",
    "\n",
    "    if start_idx >= len(lines):\n",
    "        return f\"Error: Line offset {offset} exceeds file length ({len(lines)} lines)\"\n",
    "\n",
    "    # Include line numbers, truncate very long lines\n",
    "    result_lines = []\n",
    "    for i in range(start_idx, end_idx):\n",
    "        line_content = lines[i][:2000]\n",
    "        result_lines.append(f\"{i + 1:6d}\\t{line_content}\")\n",
    "\n",
    "    return \"\\n\".join(result_lines)\n",
    "\n",
    "@tool(description=WRITE_FILE_DESCRIPTION, parse_docstring=True)\n",
    "def write_file(\n",
    "    file_path: str,\n",
    "    content: str,\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    ") -> Command:\n",
    "    \"\"\"Write content to a file in the virtual filesystem.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path where the file should be created/updated\n",
    "        content: Content to write to the file\n",
    "        state: Agent state containing virtual filesystem (injected in tool node)\n",
    "        tool_call_id: Tool call identifier for message response (injected in tool node)\n",
    "\n",
    "    Returns:\n",
    "        Command to update agent state with new file content\n",
    "    \"\"\"\n",
    "    files = state.get(\"files\", {})\n",
    "    files[file_path] = content\n",
    "    return Command(\n",
    "        update={\n",
    "            \"files\": files,\n",
    "            \"messages\": [\n",
    "                ToolMessage(f\"Updated file {file_path}\", tool_call_id=tool_call_id)\n",
    "            ],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf65a6",
   "metadata": {},
   "source": [
    "\n",
    "#### Tool 1: `ls` (List Files)\n",
    "\n",
    "**What it does**: Shows all the files the agent has saved\n",
    "\n",
    "**Human equivalent**: Looking at your folder to see what notes you've taken\n",
    "\n",
    "#### Tool 2: `read_file` (Read a File)\n",
    "\n",
    "**What it does**: \n",
    "- Reads the contents of a specific file\n",
    "- Can read just part of a file (using `offset` and `limit`) to avoid information overload\n",
    "- Adds line numbers for easy reference\n",
    "\n",
    "**Human equivalent**: Opening a specific notebook and reading your notes\n",
    "\n",
    "#### Tool 3: `write_file` (Save Information)\n",
    "\n",
    "**What it does**: \n",
    "- Creates a new file or completely replaces an existing one\n",
    "- Saves information permanently (for this conversation)\n",
    "- Returns a confirmation message\n",
    "\n",
    "**Human equivalent**: Writing notes in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c411b",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- `read_file` supports pagination (`offset`, `limit`) and line-numbering. This prevents dumping huge files into context at once.\n",
    "- `write_file` returns a `Command` that updates state and appends a `ToolMessage` so the LLM sees the operation outcome.\n",
    "- `ls` is simple: just list the keys of `files`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c34e10",
   "metadata": {},
   "source": [
    "### 4) Prompting the Agent to Use Files\n",
    "\n",
    "You craft instructions that teach the agent a workflow:\n",
    "1. Orient: check existing files with `ls()`\n",
    "2. Save: store the user‚Äôs request with `write_file()`\n",
    "3. Read: read back the saved file with `read_file()` to answer precisely\n",
    "\n",
    "Plus a simple research rule: ‚ÄúOnly use `web_search` once and base your answer on it.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567a40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_USAGE_INSTRUCTIONS = \"\"\"You MUST follow this exact workflow for every user request:\n",
    "\n",
    "1. **ALWAYS start by calling ls()** to check existing files\n",
    "2. **ALWAYS call write_file()** to save the user's request to \"user_request.txt\" \n",
    "3. **THEN call web_search()** exactly once to gather information\n",
    "4. **ALWAYS call read_file()** to re-read the saved user request\n",
    "5. **ONLY THEN provide your final answer** based on the research and saved request\n",
    "\n",
    "DO NOT skip any steps. DO NOT take shortcuts. This workflow is mandatory.\n",
    "\"\"\"\n",
    "\n",
    "SIMPLE_RESEARCH_INSTRUCTIONS = \"\"\"After completing the required file workflow above, use web_search exactly once and base your answer on both the search results and the saved user request.\"\"\"\n",
    "\n",
    "ENFORCEMENT = \"\"\"\n",
    "CRITICAL: You will be marked as FAILED if you:\n",
    "- Skip calling ls() first\n",
    "- Skip saving the user request with write_file()  \n",
    "- Skip reading back the request with read_file()\n",
    "- Use web_search without following the file workflow\n",
    "\n",
    "SUCCESS means: ls() ‚Üí write_file() ‚Üí web_search() ‚Üí read_file() ‚Üí final answer\n",
    "\"\"\"\n",
    "\n",
    "INSTRUCTIONS = FILE_USAGE_INSTRUCTIONS + \"\\n\\n\" + ENFORCEMENT + \"\\n\\n\" + SIMPLE_RESEARCH_INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448cdee",
   "metadata": {},
   "source": [
    "This is `‚Äúprompt engineering‚Äù` for tool usage: clear, step-by-step expectations reduce confusion and improve reliability.\n",
    "\n",
    "**Important note:** I changed the two prompts `\"FILE_USAGE_INSTRUCTIONS\"` and `\"SIMPLE_RESEARCH_INSTRUCTIONS\"` and also added another prompt called \n",
    "`\"ENFORCEMENT\"`. \n",
    "The reason for this was that when I ran the agent with the two initial prompts of the academy, the agent did not use the \n",
    "`read_file`, `write_file`, and `ls` tools, which means there was no `Context Offloading`. That's why I decided to change the prompts related to the \n",
    "agent workflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb1e88",
   "metadata": {},
   "source": [
    "\n",
    "### 5) Building the Agent with create_react_agent\n",
    "\n",
    "**You create:**\n",
    "- A mock `web_search` tool (returns a canned MCP summary)\n",
    "- The agent with your tools and prompt\n",
    "- `DeepAgentState` as the state schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0ed60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mock search result\n",
    "search_result = \"\"\"The Model Context Protocol (MCP) is an open standard protocol developed \n",
    "by Anthropic to enable seamless integration between AI models and external systems like \n",
    "tools, databases, and other services. It acts as a standardized communication layer, \n",
    "allowing AI models to access and utilize data from various sources in a consistent and \n",
    "efficient manner. Essentially, MCP simplifies the process of connecting AI assistants \n",
    "to external services by providing a unified language for data exchange. \"\"\"\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def web_search(query: str):\n",
    "    \"\"\"Search the web for information on a specific topic.\n",
    "\n",
    "    This tool performs web searches and returns relevant results\n",
    "    for the given query. Use this when you need to gather information from\n",
    "    the internet about any topic.\n",
    "\n",
    "    Args:\n",
    "        query: The search query string. Be specific and clear about what\n",
    "               information you're looking for.\n",
    "\n",
    "    Returns:\n",
    "        Search results from search engine.\n",
    "\n",
    "    Example:\n",
    "        web_search(\"machine learning applications in healthcare\")\n",
    "    \"\"\"\n",
    "    return search_result\n",
    "\n",
    "model = init_chat_model(model=\"openai:gpt-4o-mini\", api_key=\"sk-proj-***\", temperature=0.0)\n",
    "tools = [ls, read_file, write_file, web_search]\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model, tools, prompt=INSTRUCTIONS, state_schema=DeepAgentState\n",
    ")\n",
    "\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80123262",
   "metadata": {},
   "source": [
    "What `create_react_agent` does:\n",
    "- Builds a ReAct loop: LLM step ‚Üí tool execution ‚Üí observe ‚Üí repeat until done\n",
    "- Manages the `messages` and your `files` state\n",
    "- Executes multiple tool calls in one step if the LLM requests them\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c254a",
   "metadata": {},
   "source": [
    "\n",
    "### 6) Running the Agent (Step-by-Step Behavior)\n",
    "\n",
    "You invoke the agent with:\n",
    "- A user message: ‚ÄúGive me an overview of Model Context Protocol (MCP).‚Äù\n",
    "- Initial empty files: `{}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me an overview of Model Context Protocol (MCP).\",\n",
    "            }\n",
    "        ],\n",
    "        \"files\": {},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f519a8",
   "metadata": {},
   "source": [
    "**Typical flow observed in your sample output:**\n",
    "1. AI calls `ls` to see current files (none).\n",
    "2. AI calls `write_file` to save the user request into `user_request.txt`.\n",
    "3. AI calls `web_search` once to fetch info.\n",
    "4. AI calls `read_file` to read back the saved request (to stay aligned with the user‚Äôs ask).\n",
    "5. AI composes the final answer using the search result, tailored to the saved prompt.\n",
    "\n",
    "**Why this is good:**\n",
    "- The agent ‚Äúremembers‚Äù the exact user request in a file.\n",
    "- Even if the conversation grows, it can re-load the anchor requirements using `read_file`.\n",
    "- This mirrors production patterns where sub-agents spawn, read the shared plan, do focused work, and write artifacts or notes back to files.\n",
    "\n",
    "Inspecting the final state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8c055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools: ['ls', 'read_file', 'write_file', 'web_search']\n",
      "Instructions preview: You MUST follow this exact workflow for every user request:\n",
      "\n",
      "1. **ALWAYS start by calling ls()** to check existing files\n",
      "2. **ALWAYS call write_file()** to save the user's request to \"user_request.txt...\n",
      "=== TOOL CALLS MADE ===\n",
      "Step 1: ls with {}\n",
      "Step 3: write_file with {'file_path': 'user_request.txt', 'content': 'Give me an overview of Model Context Protocol (MCP).'}\n",
      "Step 5: web_search with {'query': 'Model Context Protocol (MCP) overview'}\n",
      "Step 7: read_file with {'file_path': 'user_request.txt'}\n",
      "\n",
      "=== FILES CREATED ===\n",
      "{'user_request.txt': 'Give me an overview of Model Context Protocol (MCP).'}\n"
     ]
    }
   ],
   "source": [
    "# Before creating agent, verify your tools\n",
    "print(\"Available tools:\", [tool.name for tool in tools])\n",
    "print(\"Instructions preview:\", INSTRUCTIONS[:200] + \"...\")\n",
    "\n",
    "# After running, check the trace\n",
    "def detailed_trace(result):\n",
    "    print(\"=== TOOL CALLS MADE ===\")\n",
    "    for i, msg in enumerate(result[\"messages\"]):\n",
    "        if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "            for call in msg.tool_calls:\n",
    "                print(f\"Step {i}: {call['name']} with {call['args']}\")\n",
    "    \n",
    "    print(\"\\n=== FILES CREATED ===\")\n",
    "    print(result.get(\"files\", {}))\n",
    "\n",
    "detailed_trace(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048f563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] üßë Human:\n",
      "    Give me an overview of Model Context Protocol (MCP).\n",
      "\n",
      "[02] ü§ñ AI (Tool Calls):\n",
      "    ‚îú‚îÄüîß Tool Call [1]: ls\n",
      "    ‚îÇ     Args: {}\n",
      "\n",
      "[03] üõ†Ô∏è Tool Output:\n",
      "    Tool: call_WxvKBI3oO0MqBA6fSHo9RVDF\n",
      "    Output: []\n",
      "\n",
      "[04] ü§ñ AI (Tool Calls):\n",
      "    ‚îú‚îÄüîß Tool Call [1]: write_file\n",
      "    ‚îÇ     Args: {'file_path': 'user_request.txt', 'content': 'Give me an overview of Model Context Protocol (MCP).'}\n",
      "\n",
      "[05] üõ†Ô∏è Tool Output:\n",
      "    Tool: call_fQR7PBh6hGllkISGo6rbaHrR\n",
      "    Output: Updated file user_request.txt\n",
      "\n",
      "[06] ü§ñ AI (Tool Calls):\n",
      "    ‚îú‚îÄüîß Tool Call [1]: web_search\n",
      "    ‚îÇ     Args: {'query': 'Model Context Protocol (MCP) overview'}\n",
      "\n",
      "[07] üõ†Ô∏è Tool Output:\n",
      "    Tool: call_V5oyZFIKGA9eCJXMALz2ikJD\n",
      "    Output: The Model Context Protocol (MCP) is an open standard protocol developed \n",
      "by Anthropic to enable seamless integration between AI models and external systems like \n",
      "tools, databases, and other services. It acts as a standardized communication layer, \n",
      "allowing AI models to access and utilize data from various sources in a consistent and \n",
      "efficient manner. Essentially, MCP simplifies the process of connecting AI assistants \n",
      "to external services by providing a unified language for data exchange. \n",
      "\n",
      "[08] ü§ñ AI (Tool Calls):\n",
      "    ‚îú‚îÄüîß Tool Call [1]: read_file\n",
      "    ‚îÇ     Args: {'file_path': 'user_request.txt'}\n",
      "\n",
      "[09] üõ†Ô∏è Tool Output:\n",
      "    Tool: call_eBuKfVPZ77QpV4bFcx1cJ4HR\n",
      "    Output:      1\tGive me an overview of Model Context Protocol (MCP).\n",
      "\n",
      "[10] ü§ñ AI (Final/Intermediate):\n",
      "    The Model Context Protocol (MCP) is an open standard protocol developed by Anthropic designed to facilitate seamless integration between AI models and external systems, such as tools, databases, and other services. It serves as a standardized communication layer, enabling AI models to access and utilize data from various sources in a consistent and efficient manner. Essentially, MCP simplifies the connection process for AI assistants to external services by providing a unified language for data exchange.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
    "\n",
    "def print_agent_trace(messages):\n",
    "    \"\"\"\n",
    "    Print a detailed step-by-step trace of the agent's reasoning, tool calls, and tool outputs,\n",
    "    showing the full \"graph\" of the agent's process.\n",
    "    \"\"\"\n",
    "    for i, message in enumerate(messages):\n",
    "        prefix = f\"[{i+1:02d}]\"\n",
    "        # Human input\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"{prefix} üßë Human:\")\n",
    "            print(f\"    {message.content}\\n\")\n",
    "        # AI message (may include tool calls)\n",
    "        elif hasattr(message, \"tool_calls\") and getattr(message, \"tool_calls\"):\n",
    "            print(f\"{prefix} ü§ñ AI (Tool Calls):\")\n",
    "            for j, call in enumerate(message.tool_calls):\n",
    "                name = call.get(\"name\", \"<unknown>\")\n",
    "                args = call.get(\"args\", {})\n",
    "                print(f\"    ‚îú‚îÄüîß Tool Call [{j+1}]: {name}\")\n",
    "                print(f\"    ‚îÇ     Args: {args}\")\n",
    "            print()\n",
    "        # Tool output\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"{prefix} üõ†Ô∏è Tool Output:\")\n",
    "            print(f\"    Tool: {getattr(message, 'tool_call_id', '<unknown>')}\")\n",
    "            print(f\"    Output: {message.content}\\n\")\n",
    "        # AI message (final answer or intermediate reasoning)\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"{prefix} ü§ñ AI (Final/Intermediate):\")\n",
    "            print(f\"    {message.content}\\n\")\n",
    "        # Fallback for any other message type\n",
    "        else:\n",
    "            content = getattr(message, \"content\", \"\")\n",
    "            if content:\n",
    "                print(f\"{prefix} ü§ñ AI (Other):\")\n",
    "                print(f\"    {content}\\n\")\n",
    "\n",
    "print_agent_trace(result[\"messages\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda03551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_request.txt': 'Give me an overview of Model Context Protocol (MCP).'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"files\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13658862",
   "metadata": {},
   "source": [
    "\n",
    "### The Workflow Strategy\n",
    "\n",
    "\n",
    "#### Step-by-Step Example:\n",
    "\n",
    "Let's trace through what happens when a user asks: *\"Give me an overview of Model Context Protocol (MCP).\"*\n",
    "\n",
    "#### Step 1: Check What Files Exist\n",
    "```\n",
    "Agent calls: ls()\n",
    "Result: [] (empty list - no files yet)\n",
    "```\n",
    "\n",
    "#### Step 2: Save the User's Request\n",
    "```\n",
    "Agent calls: write_file(\"user_request.txt\", \"User Request: Give me an overview of Model Context Protocol...\")\n",
    "Result: \"Updated file user_request.txt\"\n",
    "```\n",
    "\n",
    "**Why this is smart**: The agent saves the original question so it won't forget what the user actually asked for, even after doing lots of research.\n",
    "\n",
    "#### Step 3: Do Research\n",
    "```\n",
    "Agent calls: web_search(\"Model Context Protocol MCP overview...\")\n",
    "Result: [Information about MCP]\n",
    "```\n",
    "\n",
    "#### Step 4: Re-read Original Request\n",
    "```\n",
    "Agent calls: read_file(\"user_request.txt\")\n",
    "Result: Shows the original user request with line numbers\n",
    "```\n",
    "\n",
    "**Why this is brilliant**: Before answering, the agent checks its notes to make sure it answers exactly what the user asked for!\n",
    "\n",
    "#### Step 5: Provide Comprehensive Answer\n",
    "The agent now has:\n",
    "- The original question (from its file)\n",
    "- Research results (from web search)\n",
    "- Clear focus on what to answer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b86eb",
   "metadata": {},
   "source": [
    "### 8) Mental Model: What‚Äôs Happening Under the Hood\n",
    "\n",
    "- The agent‚Äôs state has:\n",
    "  - `messages`: conversation + tool outputs\n",
    "  - `files`: your virtual filesystem\n",
    "- Tools are standard Python functions decorated with `@tool`. The LLM can call them by name with arguments.\n",
    "- `InjectedState` gives tools access to state without exposing it to the LLM prompt.\n",
    "- `Command(update=...)` lets tools update multiple parts of state atomically (e.g., write file + add a ToolMessage).\n",
    "- The ReAct loop continues until the LLM stops requesting tools and returns a final answer.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
